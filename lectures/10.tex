%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Optimization & SVMs -- Class Notes and Lecture on deriving the Loss function 
%            : for the support vector machine for maximizing margin.
%            : University of Southern Maine 
%            : @james.quinlan
%            : @abdirahman.mohamed, Wyatt McCurdy
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{fullpage}
\usepackage{enumitem} % for outlining
\usepackage{tcolorbox} % for colored boxes

\title{Optimization \& SVMs -- Class Notes}
\author{[Abdirahman Mohamed, Wyatt McCurdy]}
\date{}

\begin{document}
\maketitle

\section*{Objectives}
\begin{outline}
    \1 Understand monotonic functions and their properties.
    \1 Learn the formulation of SVMs through maximum margin optimization.
    \1 Study Lagrange multipliers in constrained optimization.
    \1 Explore gradients, partial derivatives, and various gradient descent methods.
    \1 Understand the primal problem (objective function) of SVMs.
    \1 Understand the dual problem using Lagrange multipliers.
    \1 Practice computing the gradient.
\end{outline}

\rule[0.0051in]{\textwidth}{0.00025in}
% ----------------------------------------------------------------

\section{Mathematics and SVM Optimization}

\subsection{Monotonic Functions}
\begin{itemize}
    \item \textbf{Definition:} A function \( f(x) \) is \emph{monotonic} if whenever 
    \[
    x_1 \leq x_2, \quad \text{then} \quad f(x_1) \leq f(x_2).
    \]
    \item \textbf{Takeaway:} Lower input values yield lower output values.
\end{itemize}

\subsection{Maximum Margin in SVMs}
\begin{itemize}
    \item \textbf{Margin:} The gap between the decision boundary and the closest data points (support vectors).
    \item \textbf{Goal:} Maximize the margin, which is equivalent to maximizing 
    \[
    \frac{1}{\|w\|}.
    \]
    \item \textbf{Alternate Forms:} Minimizing any of these differentiable forms is equivalent:
    \begin{itemize}
        \item \(\|w\|\)
        \item \(\|w\|^2\)
        \item \(\frac{1}{2}\|w\|^2\)
    \end{itemize}
\end{itemize}

\subsection{SVM Optimization Setup (Primal Problem)}
\begin{itemize}
    \item \textbf{Objective:}
    \[
    \min \frac{1}{2}\|w\|^2.
    \]
    \item \textbf{Constraints:} For every sample \((x_i, y_i)\),
    \[
    y_i (w^T x_i + b) \geq 1,
    \]
    or equivalently,
    \[
    y_i (w^T x_i + b) - 1 \geq 0.
    \]
\end{itemize}

\subsection{Lagrange Multipliers and the Dual Problem}
\begin{itemize}
    \item \textbf{Purpose:} Incorporate inequality constraints into the optimization problem.
    \item \textbf{Method:} Multiply each constraint by a Lagrange multiplier \(\alpha_i \geq 0\):
    \[
    \alpha_i \bigl( y_i (w^T x_i + b) - 1 \bigr).
    \]
    \item \textbf{Lagrangian Formulation:}
    \[
    L(w, b, \alpha) = \frac{1}{2}\|w\|^2 - \sum_{i=1}^{n} \alpha_i \bigl( y_i (w^T x_i + b) - 1 \bigr).
    \]
    \item \textbf{Dual Problem:} By taking derivatives with respect to \(w\) and \(b\) and setting them to zero, we obtain:
    \[
    w = \sum_{i=1}^{n} \alpha_i y_i x_i \quad \text{and} \quad \sum_{i=1}^{n} \alpha_i y_i = 0.
    \]
    Substituting \(w\) back into the loss function leads to the dual formulation:
    \[
    \max_{\alpha} \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j x_i^T x_j.
    \]
\end{itemize}

\section{Computing the Gradient}

The loss function for SVMs is given by:
\[
L = \frac{1}{2}\|w\|^2 - \sum_{i=1}^{n} \alpha_i \bigl( y_i (w^T x_i + b) - 1 \bigr).
\]

\subsection{The Left-hand Derivative}
The derivative of \(\frac{1}{2}\|w\|^2\) with respect to \(w\) is:
\[
\frac{\partial \left(\frac{1}{2}\|w\|^2\right)}{\partial w} = w.
\]

\subsection{The Right-hand Derivative}
For the term 
\[
\sum_{i=1}^{n} \alpha_i (y_i(w^T x_i + b) - 1),
\]
the derivative with respect to \(w\) is:
\[
\frac{\partial}{\partial w} \sum_{i=1}^{n} \alpha_i y_i w^T x_i = \sum_{i=1}^{n} \alpha_i y_i x_i.
\]
Similarly, the derivative with respect to \(b\) is:
\[
\frac{\partial}{\partial b} \sum_{i=1}^{n} \alpha_i y_i b = \sum_{i=1}^{n} \alpha_i y_i.
\]

Setting these derivatives equal to zero yields:
\[
w = \sum_{i=1}^{n} \alpha_i y_i x_i \quad \text{and} \quad \sum_{i=1}^{n} \alpha_i y_i = 0.
\]

Substituting \(w\) back into the loss function gives:
\[
L = \sum_{i=1}^{n} \alpha_i - \frac{1}{2}\, w^T w,
\]
which, after replacing \(w\) with \(\sum_{i=1}^{n} \alpha_i y_i x_i\), becomes:
\[
L = \sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j x_i^T x_j.
\]

\begin{tcolorbox}
\textbf{XPL1}: Demonstrate that 
\[
\sum_{i=1}^n \alpha_i \sum_{j=1}^n \beta_j = \sum_{i=1}^n \sum_{j=1}^n \alpha_i\beta_j.
\]
\end{tcolorbox}

\section{Summary}
In these notes, we have:
\begin{itemize}
    \item Reviewed monotonic functions and the concept of maximum margin in SVMs.
    \item Set up the primal optimization problem for SVMs.
    \item Incorporated constraints using Lagrange multipliers to derive the dual problem.
    \item Computed gradients with respect to the weight vector \(w\) and bias \(b\) to establish optimality conditions.
\end{itemize}

This framework underpins many machine learning optimization techniques, including various gradient descent methods used in practice.

\end{document}
